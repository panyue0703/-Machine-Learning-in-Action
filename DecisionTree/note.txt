决策树概括：
1. 决策树是经常使用的数据挖掘算法。

2.kNN算法可以完成很多分类任务，但它最大的缺点是无法给出数据的内在含义，决策树的主要优势就在于数据形式非常容易理解；

3.决策树的一个重要任务是为了理解数据中所蕴含的知识信息，因此决策树可以使用不熟悉的数据集，并从中提取出一系列规则，这些机器根据数据集创建规则的过程，就是机器学习的过程。

4.专家系统中经常使用决策树，而且决策树给出结果往往可以匹敌在当前领域具有几十年工作经验的人类专家；

决策树原理：
根据信息增益变化，选出最优特征值，以此特征值划分数据集作为该特征值的子集，即树结构的子节点；
分别递归该划分子集过程，直到子集为同一标签类别，或特征值划分完毕，建树过程结束。
由此创建一棵树模型，然后依据此树模型来划分新的待分类向量；

决策树优缺点：
1.优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关的特征数据；
模型具有可读性，分类熟读快。

缺点：可能会产生过度匹配问题；

决策树适用数据类型：
数值型和标称型

决策树学习通常包含三个步骤：
a.特征选择:根据信息增益变化，
随机变量X的概率分布：
P(X=xj)=pi，  i=1,2,...,n
则随机变量的熵
H(X) = -sum(i=1,n,pilogpi)
若对数以2或e为底，对应熵的单位称为比特bit或纳特nat
由定义可知，熵只依赖于X的分布，与X的取值无关

条件熵：H(Y|X)表示在已知随机变量X的条件下，随机变量Y的不确定性，X给定下的Y的条件熵
H(Y|X)=SUM(i=1,n,piH(Y|X=xi) pi=P(X=xi),i-1,2...,n

信息增益 假定特征A对训练集D的信息增益g(D,A)
g(D,A) = H(D) - H(D|A)

信息增益的算法：
(1) 计算数据集D的经验熵H(d)
H(D) = -SUM(k=1,K,abs(Ck)/abs(D) log(2,abs(Ck)/abs(D)))
(2)计算特征值A对数据集D的经验条件熵H(D|A)
H(D|A)=SUM(i=1,n,abs(Di)/abs(D)*H(Di) log(2,abs(Ck)/abs(D)))

(3) 计算信息增益
g(D,A) = H(D)-H(D|A)

以信息增益划分训练数据集，存在偏向于选择取值较多的特征的问题，使用信息增益比可以矫正该问题。
特征A对训练训练数据集D的信息增益比gR(D,A)
gR(D,A) = g(D,A)/Ha(D|A)
g(D,A):信息增益
H(D|A)数据集D关于A的值的熵
 
b.决策树的生成
ID3算法的核心是在决策树各个节点上应用信息增益比选择特征，递归的构建决策树；
具体方法：从根节点开始，对节点计算所有特征的信息增益，选择信息增益比最大的作为节点特征，由该特征的
不同取值建立子节点；再对子节点递归调用以上方法，构造树；
知道所有特征的信息增益均很小或没有特征可以选择为止；

ID3算法只有树的生成，容易产生过拟合；C

c.决策树的修剪

组织杂乱无章数据的一种方法，就是使用信息论度量信息。
在划分数据集之前后，发生的变化成为信息增益，计算每个特征值划分获得信息增益，信息增益最高的特征就是最好的选择；

熵，定义为信息的期望值，信息定义为
l(xi) = -logx(p(xi))    p(xi)=该分类的概率
